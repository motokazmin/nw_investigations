{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tokenizers.pre_tokenizers import Whitespace\nfrom tokenizers.trainers import WordLevelTrainer\nfrom tokenizers import Tokenizer, models, pre_tokenizers, trainers\nfrom tokenizers.models import WordLevel\nfrom transformers import BertConfig, BertTokenizer, BertForMaskedLM, AutoModel, AutoConfig, AutoTokenizer, AutoModelForMaskedLM\nfrom transformers import Trainer, TrainingArguments\nfrom transformers import DataCollatorForLanguageModeling\nfrom transformers import LineByLineTextDataset\nfrom torch import nn\nimport torch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MLTimeseriesTrainer(Trainer):\n    def init_params_ex(self, dmin, dmax, uvalues):\n        self.dmin = dmin\n        self.dmax = dmax\n        self.uvalues = uvalues\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs['labels']\n        logits = model(**inputs)['logits']\n        indices = labels != -100\n        labels = labels[indices]\n        logits = logits[indices]\n\n        if len(logits) > 0:\n            idx = 0\n            for y_true in tokenizer.convert_ids_to_tokens(labels.view(-1).cpu().numpy()):\n                recalc_logits = [abs(el - int(y_true)) for el in self.uvalues]\n                s = np.sum(recalc_logits)\n                recalc_logits = [1, 1, 1, 1, 1] + list([np.exp(1 - el/s)/2.14 for el in recalc_logits])\n\n                results = []\n                for (r,l) in zip(recalc_logits, logits[idx].detach().cpu().numpy()):\n                    results.append(r*l)\n                    \n                logits[idx] = torch.tensor(results)\n                idx += 1\n\n        logits.reques_gradient = True\n        loss_fct = nn.CrossEntropyLoss()\n        return loss_fct(logits, labels)\n        \ndef prepare_tokenizer(uvalues, vocab_file_name):\n    !mkdir krv_tokenizer\n\n    print(f'The size of vacabulary {len(uvalues)}')\n\n    s = '[PAD]\\n[UNK]\\n[CLS]\\n[SEP]\\n[MASK]\\n'\n    for i in uvalues:\n        s += str(i) + '\\n'\n\n    textfile = open(\"krv_tokenizer/\" + vocab_file_name, \"w\")\n    textfile.write(s)\n    textfile.close()\n\n    tokenizer = Tokenizer(WordLevel())\n    tokenizer.pre_tokenizer = Whitespace()\n    trainer = WordLevelTrainer(special_tokens=[\"<unk>\", \"<s>\", \"</s>\", \"<pad>\", \"<mask>\"])\n    tokenizer.train([\"krv_tokenizer/\" + vocab_file_name], trainer)\n\n    stm = '{\"unk_token\": \"[UNK]\", \"sep_token\": \"[SEP]\", \"pad_token\": \"[PAD]\", \"cls_token\": \"[CLS]\", \"mask_token\": \"[MASK]\"}'\n    tc  = '{\"unk_token\": \"[UNK]\", \"sep_token\": \"[SEP]\", \"pad_token\": \"[PAD]\", \"cls_token\": \"[CLS]\", \"mask_token\": \"[MASK]\", \"name_or_path\": \"krv_tokenizer\"}'\n    \n    textfile = open(\"krv_tokenizer/special_tokens_map.json\", \"w\")\n    textfile.write(stm)\n    textfile.close()\n\n    textfile = open(\"krv_tokenizer/tokenizer_config.json\", \"w\")\n    textfile.write(tc)\n    textfile.close()\n\n\ndef make_sin_data(number_of_counts, window):\n    df = pd.DataFrame(np.sin(np.linspace(-2*np.pi, 2*np.pi, number_of_counts)), columns = ['x'])\n    delta = np.abs(df.diff()).min().values[0]\n    ln1 = (len(df) // window ) * window\n    df = pd.DataFrame(df[:ln1].x.values.reshape((-1, window)))\n    df.columns = [f't{i+1}' for i in range(window)]\n        \n    return df, delta\n\ndef make_ts_data(number_of_counts, window):\n    return make_sin_data(number_of_counts, window)\n\ndef make_ts_as_history(number_of_counts = 2000, window = 4):\n    df, delta = make_ts_data(number_of_counts, window)\n    dmin = df.min().min()\n    dmax = df.max().max()\n    num_uniq_words = int(np.ceil((dmax - dmin)/delta))\n    df = ((df - dmin)/delta).astype('int')\n    uvalues = np.sort(list(set(np.concatenate(df.values))))\n    df = df.astype('str')\n    df['history'] = df.agg(' '.join, axis=1)\n    return df[['history']], uvalues, delta\n\ndef make_model(tokenizer, window, dmin, dmax, first_run = True):\n    if first_run == True:\n        model = BertForMaskedLM(config=BertConfig())\n        model.resize_token_embeddings(len(tokenizer))\n    else:\n        model = BertForMaskedLM.from_pretrained('krv_model')\n        \n    model.resize_token_embeddings(len(tokenizer))\n        \n    print(f'Num of model parameters = {model.num_parameters()}')\n\n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n    )\n\n    training_args = TrainingArguments(\n        output_dir=\"./ml_ts\",\n        overwrite_output_dir=True,\n        num_train_epochs=50,\n        per_gpu_train_batch_size=4,\n        logging_steps= 200,\n        prediction_loss_only=True,\n        #learning_rate=0.0002,\n        lr_scheduler_type = \"cosine\"\n    )\n\n    trainer = MLTimeseriesTrainer(\n        model=model,\n        args=training_args,\n        data_collator=data_collator,\n        train_dataset=dataset,\n    )\n\n    trainer.init_params_ex(-1, 1, uvalues)\n    return trainer, model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"window = 20\nnumber_of_counts = 3000\n\ndf, uvalues, delta = make_ts_as_history(number_of_counts, window)\nnp.savetxt(r'corpus.txt', df['history'].values, fmt='%s')\n\nfirst_run = True\n\ntokenizer = prepare_tokenizer(uvalues, 'vocab.txt')\ntokenizer = BertTokenizer.from_pretrained('./krv_tokenizer')\n\ndataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=\"./corpus.txt\",\n    block_size=512,\n)\n\nfor i in range(0, 1):\n    trainer, model = make_model(tokenizer, window, -1, 1, first_run = first_run)\n    trainer.train()\n    model.save_pretrained('krv_model')\n    first_run = False","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = ['63145 63760 64373 64984 65592 66198 66802 67403 68001 68596 69189 69779 70366 70949 71529 72107 72680 73250 73817 [MASK]']\nencoding = tokenizer(text, return_tensors=\"pt\").to('cuda:0')\n\n# forward pass\noutputs = model(**encoding)\npreds = outputs.logits.argmax(-1)\npreds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLinks\nFileLinks(r'krv_tokenizer')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}